{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import sys\n",
    "# DATA = os.getenv('DEXNET_DATA')\n",
    "# print(DATA)\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Completion progress bar\n",
      "[                                                                                                    ]\n",
      "[####################################################################################################]\n"
     ]
    }
   ],
   "source": [
    "arrays = {}\n",
    "\n",
    "print(\"Completion progress bar\")\n",
    "toolbar_width = 100\n",
    "# setup toolbar\n",
    "sys.stdout.write(\"[%s]\\n[\" % (\" \" * toolbar_width))\n",
    "sys.stdout.flush()\n",
    "sys.stdout.write(\"\\b[\" * (toolbar_width+1)) # return to start of line, after '['\n",
    "perct_value = 0\n",
    "total_data = 87477\n",
    "i = 0\n",
    "files_dir = '/home/ashay/Downloads/dexnet_2/dexnet_2_tensor/tensors'\n",
    "for filename in os.listdir(files_dir):          \n",
    "    if filename.endswith('.npz'): \n",
    "        with np.load( files_dir + '/' + filename) as arc:\n",
    "            arrays[filename.replace('.npz', '')] = arc\n",
    "\n",
    "    perct_comp = i/total_data*toolbar_width\n",
    "    if perct_value < perct_comp:\n",
    "        sys.stdout.write(\"#\")\n",
    "        sys.stdout.flush()\n",
    "        perct_value += 1\n",
    "    i += 1\n",
    "    \n",
    "sys.stdout.write(\"]\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "# features = {}\n",
    "# for array in arrays:\n",
    "#     f = arrays[array]\n",
    "#     feature = f['arr_0.npy']\n",
    "#     features[array] = feature\n",
    "# print(features.keys())\n",
    "\n"
   ]
  },
  {
   "source": [
    "print(len(arrays))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "87477\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3, 1000, 32, 32, 1)\n(3, 1000)\n(3, 1000)\n"
     ]
    }
   ],
   "source": [
    "# #Inputs to feed into CNN\n",
    "\n",
    "#-----------x-------------------\n",
    "search_key1 = 'depth_ims_tf_table'\n",
    "aligned_imgs_x1 = np.array([val for key, val in features.items() if search_key1 in key])\n",
    "print(np.shape(aligned_imgs_x1))\n",
    "\n",
    "search_key2 = 'hand_poses'\n",
    "gripper_depths_x2 = np.array([val for key, val in features.items() if search_key2 in key])\n",
    "gripper_depths_x2 = gripper_depths_x2[:, :, 2]\n",
    "print(np.shape(gripper_depths_x2))\n",
    "\n",
    "\n",
    "#------------y------------------\n",
    "search_key3 = 'robust_ferrari_canny'\n",
    "grasp_metrics_y = np.array([val for key, val in features.items() if search_key3 in key])\n",
    "print(np.shape(grasp_metrics_y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3000, 32, 32, 1)\n(3000,)\n(3000,)\n"
     ]
    }
   ],
   "source": [
    "#--------merging-----------\n",
    "\n",
    "aligned_imgs = aligned_imgs_x1[0]\n",
    "gripper_depths = gripper_depths_x2[0]\n",
    "grasp_metrics = grasp_metrics_y[0]\n",
    "for  i in range(1,NUM_EXAMPLES):   #replace the limit with the number of examples\n",
    "    #-----------------x1------------------\n",
    "    aligned_imgs = np.concatenate([aligned_imgs,aligned_imgs_x1[i]])\n",
    "    #-----------------x2-------------------\n",
    "    gripper_depths = np.concatenate([gripper_depths,gripper_depths_x2[i]])\n",
    "    #------------------y---------------------\n",
    "    grasp_metrics = np.concatenate([grasp_metrics,grasp_metrics_y[i]])\n",
    "\n",
    "print(np.shape(aligned_imgs))\n",
    "print(np.shape(gripper_depths))\n",
    "print(np.shape(grasp_metrics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************One hot encoding***********************\n",
    "\n",
    "# print(grasp_metrics[2])\n",
    "def one_hot_encoding(grasp_metrics):\n",
    "    for i in range(1000):\n",
    "        if(grasp_metrics[i] > 0.002):  #threshold value is 0.002 readme\n",
    "            grasp_metrics[i] = 1\n",
    "        else:\n",
    "            grasp_metrics[i] = 0\n",
    "    return grasp_metrics\n",
    "\n",
    "grasp_metrics = one_hot_encoding(grasp_metrics)\n",
    "\n",
    "# print(grasp_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import random\n",
    "\n",
    "# example of defining the discriminator model\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras import layers\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Lambda\n",
    "from tensorflow.nn import local_response_normalization\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "# from tensorflow.keras import layers\n",
    "from keras.layers import Dense, Input\n",
    "\n",
    "\n",
    "#TODO: Initialize weights in layers according to the paper https://arxiv.org/pdf/1703.09312.pdf\n",
    "def getGraspQualityVariable():\n",
    "    input = Input(shape=(32, 32, 1), name=\"img\")\n",
    "    x1 = Conv2D(filters=64, kernel_size=7, activation='relu')(input)\n",
    "    x2 = Conv2D(filters=64, kernel_size=5, activation='relu')(x1)\n",
    "    x3 = Lambda(local_response_normalization)(x2)\n",
    "    x4 = MaxPooling2D(pool_size=(2, 2), strides=2)(x3)\n",
    "    x5 = Conv2D(filters=64, kernel_size=3, activation='relu')(x4)\n",
    "    # x6 = Dropout(0.3)(x5)\n",
    "    x6 = Conv2D(filters=64, kernel_size=3, activation='relu')(x5)\n",
    "    x7 = Lambda(local_response_normalization)(x6)\n",
    "    x8 = Flatten()(x7)\n",
    "    x9 = Dense(1024, activation='relu')(x8)\n",
    "    \n",
    "    # plot_model(model, to_file='GraspQualityModel_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    return x9, input\n",
    "    \n",
    "getGraspQualityVariable()\n",
    "\n",
    "\n",
    "def getPointcloudModel():\n",
    "    input = Input(shape=(1), name=\"z\")\n",
    "    x = Dense(16, input_dim=1, activation='relu')(input)\n",
    "    return x, input\n",
    "\n",
    "\n",
    "from keras.layers.merge import concatenate\n",
    "def getDexnet2Model():\n",
    "    grasp_model, input_1 = getGraspQualityVariable()\n",
    "    pc_model, input_2 = getPointcloudModel()\n",
    "\n",
    "    out = Dense(1024, activation='relu')\n",
    "\n",
    "    # x = layers.concatenate([grasp_model, pc_model])\n",
    "    merge = concatenate([grasp_model, pc_model])\n",
    "    out_1 = Dense(1024, activation='relu')(merge)\n",
    "    out_2 = Dense(2, , activation='softmax')(out_1)\n",
    "    model = Model(inputs=[input_1, input_2], outputs=out_2)\n",
    "    plot_model(model, to_file='Dexnet2_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "first_model = getDexnet2Model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_5\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nimg (InputLayer)                [(None, 32, 32, 1)]  0                                            \n__________________________________________________________________________________________________\nconv2d_44 (Conv2D)              (None, 26, 26, 64)   3200        img[0][0]                        \n__________________________________________________________________________________________________\nconv2d_45 (Conv2D)              (None, 22, 22, 64)   102464      conv2d_44[0][0]                  \n__________________________________________________________________________________________________\nlambda_22 (Lambda)              (None, 22, 22, 64)   0           conv2d_45[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_11 (MaxPooling2D) (None, 11, 11, 64)   0           lambda_22[0][0]                  \n__________________________________________________________________________________________________\nconv2d_46 (Conv2D)              (None, 9, 9, 64)     36928       max_pooling2d_11[0][0]           \n__________________________________________________________________________________________________\nconv2d_47 (Conv2D)              (None, 7, 7, 64)     36928       conv2d_46[0][0]                  \n__________________________________________________________________________________________________\nlambda_23 (Lambda)              (None, 7, 7, 64)     0           conv2d_47[0][0]                  \n__________________________________________________________________________________________________\nflatten_11 (Flatten)            (None, 3136)         0           lambda_23[0][0]                  \n__________________________________________________________________________________________________\nz (InputLayer)                  [(None, 1)]          0                                            \n__________________________________________________________________________________________________\ndense_31 (Dense)                (None, 1024)         3212288     flatten_11[0][0]                 \n__________________________________________________________________________________________________\ndense_32 (Dense)                (None, 16)           32          z[0][0]                          \n__________________________________________________________________________________________________\nconcatenate_5 (Concatenate)     (None, 1040)         0           dense_31[0][0]                   \n                                                                 dense_32[0][0]                   \n__________________________________________________________________________________________________\ndense_34 (Dense)                (None, 1024)         1065984     concatenate_5[0][0]              \n__________________________________________________________________________________________________\ndense_35 (Dense)                (None, 2)            2050        dense_34[0][0]                   \n==================================================================================================\nTotal params: 4,459,874\nTrainable params: 4,459,874\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "first_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "141/141 [==============================] - 8s 52ms/step - loss: 0.9583 - accuracy: 0.7677 - val_loss: 0.6931 - val_accuracy: 0.6360\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69315, saving model to /home/firefly/dexnet/dexnet2_git/Weights/second_model.weights.best.hdf5\n",
      "--- 7.8306121826171875 seconds ---\n",
      "--- 0.1305102030436198 minutes ---\n",
      "--- 0.0021751700507269965 hours ---\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "start_time = time.time()\n",
    "checkpointer = ModelCheckpoint(filepath='/home/firefly/dexnet/dexnet2_git/Weights/second_model.weights.best.hdf5', verbose = 1, save_best_only=True)\n",
    "x_train = [aligned_imgs, gripper_depths]                                   #Check model.summary() in previous section and check model architecture in paper\n",
    "y_train = grasp_metrics                                                    #valued between [0, 1] grasp robustness for the given grasp\n",
    "first_model.fit(x_train,\n",
    "          y_train,\n",
    "          batch_size=16,                                                   #worth ecperimenting\n",
    "          epochs=1,                                      \n",
    "          validation_split=0.25,                                            #Decide on a number\n",
    "          callbacks=[checkpointer])        \n",
    "#CHange the filepath of the checkpointer varianble to store different version of weights\n",
    "elapsed_time = (time.time() - start_time)\n",
    "print(f\"--- {elapsed_time} seconds ---\")\n",
    "print(f\"--- {elapsed_time/60} minutes ---\")\n",
    "print(f\"--- {elapsed_time/3600} hours ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:5 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8faefab790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-16.139938, -35.90434 ],\n",
       "       [-16.138353, -35.8999  ],\n",
       "       [-16.151684, -35.934444],\n",
       "       [-16.137676, -35.89809 ],\n",
       "       [-16.135368, -35.89767 ],\n",
       "       [-16.14027 , -35.906994],\n",
       "       [-16.136408, -35.89941 ],\n",
       "       [-16.156223, -35.944447],\n",
       "       [-16.142263, -35.911503],\n",
       "       [-16.136065, -35.89853 ]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 223
    }
   ],
   "source": [
    "first_model.predict([aligned_imgs[290:300],gripper_depths[290:300]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])"
      ]
     },
     "metadata": {},
     "execution_count": 224
    }
   ],
   "source": [
    "grasp_metrics[290:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}