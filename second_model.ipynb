{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "# DATA = os.getenv('DEXNET_DATA')\n",
    "# print(DATA)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['robust_ferrari_canny_00000', 'robust_ferrari_canny_00001', 'hand_poses_00001', 'depth_ims_tf_table_00001', 'hand_poses_00000', 'depth_ims_tf_table_00000'])\n"
     ]
    }
   ],
   "source": [
    "arrays = {}\n",
    "datapoint = '_05590.npz'                #Enter datapoint here (anywhere between _00000 and _06728, there are a total of 6.7 million datapoints i.e, 6728 x 1000). Change this value to just '.npz' to train on entire dataset\n",
    "for filename in os.listdir('/home/firefly/dexnet/dexnet_2/dexnet_2_tensor/tensors/'):\n",
    "    # data = '_05590.npz'                \n",
    "    for npz_file_number in range (2):   #Depending on number of examples just change the range.\n",
    "        data = '_{0:05}.npz'.format(npz_file_number) \n",
    "      \n",
    "        if filename.endswith(data):\n",
    "            arrays[filename.replace('.npz', '')] = np.load( '/home/firefly/dexnet/dexnet_2/dexnet_2_tensor/tensors/' + filename)\n",
    "            #print(arrays)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "features = {}\n",
    "for array in arrays:\n",
    "    f = arrays[array]\n",
    "    feature = f['arr_0.npy']\n",
    "    features[array] = feature\n",
    "print(features.keys())\n",
    "\n"
   ]
  },
  {
   "source": [
    "print(data)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2, 1000, 32, 32, 1)\n(2, 1000)\n(2, 1000)\n"
     ]
    }
   ],
   "source": [
    "# #Inputs to feed into CNN\n",
    "\n",
    "#-----------x-------------------\n",
    "search_key1 = 'depth_ims_tf_table'\n",
    "aligned_imgs_x1 = np.array([val for key, val in features.items() if search_key1 in key])\n",
    "print(np.shape(aligned_imgs_x1))\n",
    "\n",
    "search_key2 = 'hand_poses'\n",
    "gripper_depths_x2 = np.array([val for key, val in features.items() if search_key2 in key])\n",
    "gripper_depths_x2 = gripper_depths_x2[:, :, 2]\n",
    "print(np.shape(gripper_depths_x2))\n",
    "\n",
    "\n",
    "#------------y------------------\n",
    "search_key3 = 'robust_ferrari_canny'\n",
    "grasp_metrics_y = np.array([val for key, val in features.items() if search_key3 in key])\n",
    "print(np.shape(grasp_metrics_y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Cannot interpret '1' as a data type",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-7278de4ce1b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maligned_imgs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maligned_imgs_x1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maligned_imgs_x1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maligned_imgs_x1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maligned_imgs_x1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maligned_imgs_x1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maligned_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maligned_imgs_x1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0maligned_imgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maligned_imgs_x1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret '1' as a data type"
     ]
    }
   ],
   "source": [
    "aligned_imgs=np.zeros([aligned_imgs_x1.shape[0], aligned_imgs_x1.shape[1], aligned_imgs_x1.shape[2], aligned_imgs_x1.shape[3]], aligned_imgs_x1.shape[4])\n",
    "print(aligned_imgs.shape)\n",
    "for i in range(len(aligned_imgs_x1)):\n",
    "    aligned_imgs[i, :, :, :, :] = aligned_imgs_x1[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-5353d1f93192>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrasp_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mgrasp_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrasp_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# print(grasp_metrics)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-5353d1f93192>\u001b[0m in \u001b[0;36mone_hot_encoding\u001b[0;34m(grasp_metrics)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mone_hot_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrasp_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrasp_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.002\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#threshold value is 0.002 readme\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mgrasp_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "#******************One hot encoding***********************\n",
    "\n",
    "# print(grasp_metrics[2])\n",
    "def one_hot_encoding(grasp_metrics):\n",
    "    for i in range(1000):\n",
    "        if(grasp_metrics[i] > 0.002):  #threshold value is 0.002 readme\n",
    "            grasp_metrics[i] = 1\n",
    "        else:\n",
    "            grasp_metrics[i] = 0\n",
    "    return grasp_metrics\n",
    "\n",
    "grasp_metrics = one_hot_encoding(grasp_metrics)\n",
    "\n",
    "# print(grasp_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import random\n",
    "\n",
    "# example of defining the discriminator model\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras import layers\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Lambda\n",
    "from tensorflow.nn import local_response_normalization\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "# from tensorflow.keras import layers\n",
    "from keras.layers import Dense, Input\n",
    "\n",
    "\n",
    "#TODO: Initialize weights in layers according to the paper https://arxiv.org/pdf/1703.09312.pdf\n",
    "def getGraspQualityVariable():\n",
    "    input = Input(shape=(32, 32, 1), name=\"img\")\n",
    "    x1 = Conv2D(filters=64, kernel_size=7, activation='relu')(input)\n",
    "    x2 = Conv2D(filters=64, kernel_size=5, activation='relu')(x1)\n",
    "    x3 = Lambda(local_response_normalization)(x2)\n",
    "    x4 = MaxPooling2D(pool_size=(2, 2), strides=2)(x3)\n",
    "    x5 = Conv2D(filters=64, kernel_size=3, activation='relu')(x4)\n",
    "    # x6 = Dropout(0.3)(x5)\n",
    "    x6 = Conv2D(filters=64, kernel_size=3, activation='relu')(x5)\n",
    "    x7 = Lambda(local_response_normalization)(x6)\n",
    "    x8 = Flatten()(x7)\n",
    "    x9 = Dense(1024, activation='relu')(x8)\n",
    "    \n",
    "    # plot_model(model, to_file='GraspQualityModel_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    return x9, input\n",
    "    \n",
    "getGraspQualityVariable()\n",
    "\n",
    "\n",
    "def getPointcloudModel():\n",
    "    input = Input(shape=(1), name=\"z\")\n",
    "    x = Dense(16, input_dim=1, activation='relu')(input)\n",
    "    return x, input\n",
    "\n",
    "\n",
    "from keras.layers.merge import concatenate\n",
    "def getDexnet2Model():\n",
    "    grasp_model, input_1 = getGraspQualityVariable()\n",
    "    pc_model, input_2 = getPointcloudModel()\n",
    "\n",
    "    out = Dense(1024, activation='relu')\n",
    "\n",
    "    # x = layers.concatenate([grasp_model, pc_model])\n",
    "    merge = concatenate([grasp_model, pc_model])\n",
    "    out_1 = Dense(1024, activation='relu')(merge)\n",
    "    out_2 = Dense(2, activation='softmax')(out_1)\n",
    "    model = Model(inputs=[input_1, input_2], outputs=out_2)\n",
    "    plot_model(model, to_file='Dexnet2_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "first_model = getDexnet2Model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_2\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nimg (InputLayer)                [(None, 32, 32, 1)]  0                                            \n__________________________________________________________________________________________________\nconv2d_16 (Conv2D)              (None, 26, 26, 64)   3200        img[0][0]                        \n__________________________________________________________________________________________________\nconv2d_17 (Conv2D)              (None, 22, 22, 64)   102464      conv2d_16[0][0]                  \n__________________________________________________________________________________________________\nlambda_8 (Lambda)               (None, 22, 22, 64)   0           conv2d_17[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_4 (MaxPooling2D)  (None, 11, 11, 64)   0           lambda_8[0][0]                   \n__________________________________________________________________________________________________\nconv2d_18 (Conv2D)              (None, 9, 9, 64)     36928       max_pooling2d_4[0][0]            \n__________________________________________________________________________________________________\nconv2d_19 (Conv2D)              (None, 7, 7, 64)     36928       conv2d_18[0][0]                  \n__________________________________________________________________________________________________\nlambda_9 (Lambda)               (None, 7, 7, 64)     0           conv2d_19[0][0]                  \n__________________________________________________________________________________________________\nflatten_4 (Flatten)             (None, 3136)         0           lambda_9[0][0]                   \n__________________________________________________________________________________________________\nz (InputLayer)                  [(None, 1)]          0                                            \n__________________________________________________________________________________________________\ndense_12 (Dense)                (None, 1024)         3212288     flatten_4[0][0]                  \n__________________________________________________________________________________________________\ndense_13 (Dense)                (None, 16)           32          z[0][0]                          \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 1040)         0           dense_12[0][0]                   \n                                                                 dense_13[0][0]                   \n__________________________________________________________________________________________________\ndense_15 (Dense)                (None, 1024)         1065984     concatenate_2[0][0]              \n__________________________________________________________________________________________________\ndense_16 (Dense)                (None, 2)            2050        dense_15[0][0]                   \n==================================================================================================\nTotal params: 4,459,874\nTrainable params: 4,459,874\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "first_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "47/47 [==============================] - 3s 52ms/step - loss: 0.4035 - accuracy: 0.8497 - val_loss: 0.2669 - val_accuracy: 0.9320\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26694, saving model to /home/firefly/dexnet/dexnet2_git/Weights/second_model.weights.best.hdf5\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 2s 50ms/step - loss: 0.3206 - accuracy: 0.9135 - val_loss: 0.2523 - val_accuracy: 0.9320\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26694 to 0.25226, saving model to /home/firefly/dexnet/dexnet2_git/Weights/second_model.weights.best.hdf5\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 2s 50ms/step - loss: 0.2786 - accuracy: 0.9258 - val_loss: 0.2533 - val_accuracy: 0.9320\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.25226\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 2s 49ms/step - loss: 0.3146 - accuracy: 0.9080 - val_loss: 0.2480 - val_accuracy: 0.9320\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.25226 to 0.24800, saving model to /home/firefly/dexnet/dexnet2_git/Weights/second_model.weights.best.hdf5\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 2s 52ms/step - loss: 0.2671 - accuracy: 0.9248 - val_loss: 0.2569 - val_accuracy: 0.9320\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.24800\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 2s 51ms/step - loss: 0.2976 - accuracy: 0.9127 - val_loss: 0.2476 - val_accuracy: 0.9320\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.24800 to 0.24759, saving model to /home/firefly/dexnet/dexnet2_git/Weights/second_model.weights.best.hdf5\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 2s 51ms/step - loss: 0.3033 - accuracy: 0.9093 - val_loss: 0.2481 - val_accuracy: 0.9320\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.24759\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 2s 50ms/step - loss: 0.2619 - accuracy: 0.9278 - val_loss: 0.2531 - val_accuracy: 0.9320\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.24759\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 2s 51ms/step - loss: 0.2898 - accuracy: 0.9157 - val_loss: 0.2474 - val_accuracy: 0.9320\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.24759 to 0.24742, saving model to /home/firefly/dexnet/dexnet2_git/Weights/second_model.weights.best.hdf5\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 2s 50ms/step - loss: 0.2998 - accuracy: 0.9116 - val_loss: 0.2475 - val_accuracy: 0.9320\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.24742\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2a902c5430>"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='/home/firefly/dexnet/dexnet2_git/Weights/second_model.weights.best.hdf5', verbose = 1, save_best_only=True)\n",
    "x_train = [aligned_imgs, gripper_depths]                                   #Check model.summary() in previous section and check model architecture in paper\n",
    "y_train = grasp_metrics                                                    #valued between [0, 1] grasp robustness for the given grasp\n",
    "first_model.fit(x_train,\n",
    "          y_train,\n",
    "          batch_size=16,                                                   #worth ecperimenting\n",
    "          epochs=5,                                      \n",
    "          validation_split=0.25,                                            #Decide on a number\n",
    "          callbacks=[checkpointer])        \n",
    "\n",
    "                                #CHange the filepath of the checkpointer varianble to store different version of weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 1.57385473e-18],\n",
       "       [1.00000000e+00, 9.76254430e-15],\n",
       "       [1.00000000e+00, 4.45226313e-29],\n",
       "       [1.00000000e+00, 8.44479286e-20],\n",
       "       [1.00000000e+00, 1.93126236e-20],\n",
       "       [1.00000000e+00, 6.48304130e-31],\n",
       "       [5.87066129e-09, 1.00000000e+00],\n",
       "       [1.00000000e+00, 4.43832960e-19],\n",
       "       [1.00000000e+00, 2.50874625e-14],\n",
       "       [1.00000000e+00, 9.92938340e-29],\n",
       "       [1.00000000e+00, 6.03239298e-19],\n",
       "       [1.00000000e+00, 2.84128119e-21],\n",
       "       [1.00000000e+00, 1.05569032e-31],\n",
       "       [2.02726120e-08, 1.00000000e+00],\n",
       "       [1.00000000e+00, 4.37048438e-20],\n",
       "       [1.00000000e+00, 2.50651388e-13],\n",
       "       [1.00000000e+00, 8.49463526e-30],\n",
       "       [1.00000000e+00, 6.00779469e-20],\n",
       "       [1.00000000e+00, 6.42472595e-21],\n",
       "       [1.00000000e+00, 9.09340154e-32],\n",
       "       [2.52236476e-09, 1.00000000e+00],\n",
       "       [1.00000000e+00, 5.52493641e-20],\n",
       "       [1.00000000e+00, 2.65539527e-14],\n",
       "       [1.00000000e+00, 3.75436209e-29],\n",
       "       [1.00000000e+00, 1.55170790e-18],\n",
       "       [1.00000000e+00, 6.76259670e-16],\n",
       "       [1.00000000e+00, 1.78224986e-31],\n",
       "       [1.44037315e-08, 1.00000000e+00],\n",
       "       [1.00000000e+00, 1.87353439e-15],\n",
       "       [1.00000000e+00, 3.15208661e-13],\n",
       "       [1.00000000e+00, 2.64010473e-28],\n",
       "       [1.00000000e+00, 6.19794149e-18],\n",
       "       [1.00000000e+00, 3.09980118e-19],\n",
       "       [1.00000000e+00, 1.59305851e-31],\n",
       "       [1.13993330e-08, 1.00000000e+00],\n",
       "       [1.00000000e+00, 1.55112795e-18],\n",
       "       [1.00000000e+00, 2.05573710e-15],\n",
       "       [1.00000000e+00, 4.78299917e-29],\n",
       "       [1.00000000e+00, 4.73858603e-18],\n",
       "       [1.00000000e+00, 3.43405315e-16],\n",
       "       [1.00000000e+00, 3.59420587e-31],\n",
       "       [2.53153187e-09, 1.00000000e+00],\n",
       "       [1.00000000e+00, 6.88101463e-19],\n",
       "       [1.00000000e+00, 5.45906255e-15],\n",
       "       [1.00000000e+00, 9.34823946e-29],\n",
       "       [1.00000000e+00, 4.62572582e-19],\n",
       "       [1.00000000e+00, 3.10367364e-17],\n",
       "       [1.00000000e+00, 3.26281379e-31],\n",
       "       [1.63091107e-09, 1.00000000e+00],\n",
       "       [1.00000000e+00, 4.06641724e-17],\n",
       "       [1.00000000e+00, 2.87388453e-14],\n",
       "       [1.00000000e+00, 7.13638716e-29],\n",
       "       [1.00000000e+00, 2.60142864e-18],\n",
       "       [1.00000000e+00, 2.76125005e-20],\n",
       "       [1.00000000e+00, 2.30383492e-31],\n",
       "       [3.23466876e-09, 1.00000000e+00],\n",
       "       [1.00000000e+00, 1.68033185e-19],\n",
       "       [1.00000000e+00, 1.06465333e-14],\n",
       "       [1.00000000e+00, 5.42957388e-29],\n",
       "       [1.00000000e+00, 5.40478571e-18]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "first_model.predict([aligned_imgs[290:350],gripper_depths[290:350]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}