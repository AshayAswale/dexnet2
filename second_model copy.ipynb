{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "from keras.models import Sequential\n",
    "import fnmatch\n",
    "# from keras import backend as K\n",
    "# K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "DATA = os.getenv('DEXNET_DATA')\n",
    "print(DATA)\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 1501853929446138523\n]\nNum GPUs Available:  0\n/home/firefly/dexnet/dexnet_2/dexnet_2_tensor/tensors/\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import random\n",
    "\n",
    "# example of defining the discriminator model\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras import layers\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Lambda\n",
    "from tensorflow.nn import local_response_normalization\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "# from tensorflow.keras import layers\n",
    "from keras.layers import Dense, Input\n",
    "\n",
    "\n",
    "#TODO: Initialize weights in layers according to the paper https://arxiv.org/pdf/1703.09312.pdf\n",
    "def getGraspQualityVariable():\n",
    "    input = Input(shape=(32, 32, 1), name=\"img\")\n",
    "    x1 = Conv2D(filters=64, kernel_size=7, activation='relu')(input)\n",
    "    x2 = Conv2D(filters=64, kernel_size=5, activation='relu')(x1)\n",
    "    x3 = Lambda(local_response_normalization)(x2)\n",
    "    x4 = MaxPooling2D(pool_size=(2, 2), strides=2)(x3)\n",
    "    x5 = Conv2D(filters=64, kernel_size=3, activation='relu')(x4)\n",
    "    # x6 = Dropout(0.3)(x5)\n",
    "    x6 = Conv2D(filters=64, kernel_size=3, activation='relu')(x5)\n",
    "    x7 = Lambda(local_response_normalization)(x6)\n",
    "    x8 = Flatten()(x7)\n",
    "    x9 = Dense(1024, activation='relu')(x8)\n",
    "    \n",
    "    # plot_model(model, to_file='GraspQualityModel_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    return x9, input\n",
    "    \n",
    "getGraspQualityVariable()\n",
    "\n",
    "\n",
    "def getPointcloudModel():\n",
    "    input = Input(shape=(1), name=\"z\")\n",
    "    x = Dense(16, input_dim=1, activation='relu')(input)\n",
    "    return x, input\n",
    "\n",
    "\n",
    "from keras.layers.merge import concatenate\n",
    "def getDexnet2Model():\n",
    "    grasp_model, input_1 = getGraspQualityVariable()\n",
    "    pc_model, input_2 = getPointcloudModel()\n",
    "\n",
    "    out = Dense(1024, activation='relu')\n",
    "\n",
    "    # x = layers.concatenate([grasp_model, pc_model])\n",
    "    merge = concatenate([grasp_model, pc_model])\n",
    "    out_1 = Dense(1024, activation='relu')(merge)\n",
    "    out_2 = Dense(2, activation='softmax')(out_1)\n",
    "    model = Model(inputs=[input_1, input_2], outputs=out_2)\n",
    "    plot_model(model, to_file='Dexnet2_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['depth_ims_tf_table_00010.npz' 'depth_ims_tf_table_00011.npz'\n 'depth_ims_tf_table_00012.npz' 'depth_ims_tf_table_00013.npz'\n 'depth_ims_tf_table_00014.npz']\n['hand_poses_00010.npz' 'hand_poses_00011.npz' 'hand_poses_00012.npz'\n 'hand_poses_00013.npz' 'hand_poses_00014.npz']\n['robust_ferrari_canny_00010.npz' 'robust_ferrari_canny_00011.npz'\n 'robust_ferrari_canny_00012.npz' 'robust_ferrari_canny_00013.npz'\n 'robust_ferrari_canny_00014.npz']\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-11bce64e6135>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mmerged_X1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mmerged_X1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_0.npy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "\n",
    "aligned_imgs_files = []\n",
    "for filename in os.listdir(DATA):\n",
    "    if fnmatch.fnmatch(filename, 'depth_ims_tf_table_*.npz'):\n",
    "        aligned_imgs_files.append(filename)  # collected all the depth_ims_tf_table files in one list\n",
    "aligned_imgs_files = np.sort(aligned_imgs_files)\n",
    "print(aligned_imgs_files[10:15])\n",
    "\n",
    "gripper_depths_files = []\n",
    "for filename in os.listdir(DATA):\n",
    "    if fnmatch.fnmatch(filename, 'hand_poses_*.npz'):\n",
    "        gripper_depths_files.append(filename)  # collected all hand_poses the files in one list\n",
    "gripper_depths_files = np.sort(gripper_depths_files)\n",
    "print(gripper_depths_files[10:15])\n",
    "\n",
    "grasp_metrics_files = []\n",
    "for filename in os.listdir(DATA):\n",
    "    if fnmatch.fnmatch(filename, 'robust_ferrari_canny_*.npz'):\n",
    "        grasp_metrics_files.append(filename)  # collected all the grasp_metrics files in one list\n",
    "grasp_metrics_files = np.sort(grasp_metrics_files)\n",
    "print(grasp_metrics_files[10:15])\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------#    \n",
    " \n",
    "for i in range (10):\n",
    "\n",
    "    X1 = [np.load(DATA + filename) for fname in aligned_imgs_files[100*i:100*(i+1)]]\n",
    "    \n",
    "    X2 = [np.load(DATA + filename) for fname in gripper_depths_files[100*i:100*(i+1)]]\n",
    "    \n",
    "    Y  = [np.load(DATA + filename) for fname in grasp_metrics_files[100*i:110*(i+1)]]\n",
    "#######################      \n",
    "    merged_X1 = []\n",
    "    for data in X1:\n",
    "        merged_X1.append(X1['arr_0.npy'])\n",
    "\n",
    "    del data\n",
    "\n",
    "    concatenated_merged_X1 = merged_X1[0]\n",
    "    for  c in range(1,100): \n",
    "            \n",
    "        concatenated_merged_X1 = np.concatenate([concatenated_merged_X1,merged_X1[c]])\n",
    "        print(c)\n",
    "        print(np.shape(concatenated_merged_X1))\n",
    "    \n",
    "    del merged_X1\n",
    "\n",
    "########################\n",
    "    merged_X2 = []\n",
    "    for data in X1:\n",
    "        merged_X2.append(X1['arr_0.npy'])\n",
    "\n",
    "    del data\n",
    "\n",
    "    concatenated_merged_X2 = merged_X2[0]\n",
    "    for  c in range(1,100): \n",
    "            \n",
    "        concatenated_merged_X2 = np.concatenate([concatenated_merged_X2,merged_X2[c]])\n",
    "        print(c)\n",
    "        print(np.shape(concatenated_merged_X2))\n",
    "    \n",
    "    del merged_X2\n",
    "#######################\n",
    "    merged_Y = []\n",
    "    for data in Y:\n",
    "        merged_Y.append(Y['arr_0.npy'])\n",
    "\n",
    "    del data\n",
    "\n",
    "    concatenated_merged_Y = merged_Y[0]\n",
    "    for  c in range(1,100): \n",
    "            \n",
    "        concatenated_merged_Y = np.concatenate([concatenated_merged_Y,merged_Y[c]])\n",
    "        print(c)\n",
    "        print(np.shape(concatenated_merged_Y))\n",
    "    \n",
    "    del merged_Y\n",
    "\n",
    "\n",
    "    aligned_imgs  = concatenated_merged_X1\n",
    "    gripper_depths = concatenated_merged_X2\n",
    "    grasp_metrics = concatenated_merged_Y\n",
    "\n",
    "\n",
    "    #******************One hot encoding***********************\n",
    "\n",
    "    # print(grasp_metrics[2])\n",
    "    def one_hot_encoding(grasp_metrics):\n",
    "        for i in range(1000):\n",
    "            if(grasp_metrics[i] > 0.002):  #threshold value is 0.002 readme\n",
    "                grasp_metrics[i] = 1\n",
    "            else:\n",
    "                grasp_metrics[i] = 0\n",
    "        return grasp_metrics\n",
    "\n",
    "    grasp_metrics = one_hot_encoding(grasp_metrics)\n",
    "\n",
    "    #------------------------------------------------------------------------------------------#\n",
    "\n",
    "    first_model = getDexnet2Model()\n",
    "\n",
    "    first_model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "    start_time = time.time()\n",
    "    checkpointer = ModelCheckpoint(filepath='/home/firefly/dexnet/dexnet2_git/Weights/second_model.weights.best.hdf5', verbose = 1, save_best_only=True)\n",
    "    x_train = [aligned_imgs, gripper_depths]                                   #Check model.summary() in previous section and check model architecture in paper\n",
    "    y_train = grasp_metrics                                                    #valued between [0, 1] grasp robustness for the given grasp\n",
    "    first_model.fit(x_train,\n",
    "            y_train,\n",
    "            batch_size=8,                                                   #worth ecperimenting\n",
    "            epochs=10,                                      \n",
    "            validation_split=0.25,                                            #Decide on a number\n",
    "            callbacks=[checkpointer])        \n",
    "    #CHange the filepath of the checkpointer varianble to store different version of weights\n",
    "    elapsed_time = (time.time() - start_time)\n",
    "    print(f\"--- {elapsed_time} seconds ---\")\n",
    "    print(f\"--- {elapsed_time/60} minutes ---\")\n",
    "    print(f\"--- {elapsed_time/3600} hours ---\")\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "grasp_metrics[290:300]"
   ]
  }
 ]
}